{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3680aaba-6702-4854-aee9-66468e2f6027",
   "metadata": {},
   "source": [
    "## This notebook is used to run hyperparameter optimization studies with Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f10e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kmenear/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import joblib\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from hpc_runtime_prediction.operation_support import train_test_split\n",
    "from hpc_runtime_prediction.data_preprocessing import label_encode_columns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbcb79a4-f0e1-4593-ad00-c3d31c8c93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dfs():\n",
    "    # Import pkl file\n",
    "    filepath = os.path.join('../data/', 'eagle_data_anonymized_20230222.pkl')\n",
    "    eagle_df = pd.read_pickle(filepath)\n",
    "    \n",
    "    categorical_features = ['user','partition']\n",
    "    label_encode_columns(eagle_df, categorical_features)\n",
    "    \n",
    "    start_time = eagle_df.submit_time.min()\n",
    "    end_time = eagle_df.submit_time.max()\n",
    "    split_times = pd.date_range(start_time, end_time, periods=22)[1:21]\n",
    "    \n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "    for split_time in split_times:\n",
    "        train_df, test_df = train_test_split(eagle_df, split_time, training_window=100, testing_window=1)\n",
    "        train_dfs.append(train_df)\n",
    "        test_dfs.append(test_df)\n",
    "    \n",
    "    return train_dfs, test_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2dcb1c7-f2c6-4b56-97ad-d27dc4aaee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs, test_dfs = get_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc69ea0-f774-4adf-b5af-d31c8aa11733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(i, train_dfs, test_dfs):\n",
    "    features = ['wallclock_req','processors_req','mem_req','nodes_req','gpus_req','user','partition']\n",
    "    target = 'run_time'\n",
    "    X_train = train_dfs[i][features]\n",
    "    y_train = train_dfs[i][target]\n",
    "    X_test = test_dfs[i][features]\n",
    "    y_test = test_dfs[i][target]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4974459-d3d2-48e2-8a29-cb2a0c2f30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to fit and evaluate a model with given hyperparameters on a single dataset \n",
    "def fit_model(params):\n",
    "    params['eval_metric'] = 'rmse'\n",
    "    #n_estimators = params['n_estimators']\n",
    "    rmse_list = list()\n",
    "    eval_name = 'val'\n",
    "    for i in range(len(train_dfs)):\n",
    "        X_train, y_train, X_test, y_test = load_data(i, train_dfs, test_dfs)\n",
    "        #dtrain = xgb.DMatrix(data=X_train,\n",
    "        #                     label=y_train)\n",
    "        #dval = xgb.DMatrix(data=X_test,\n",
    "        #                     label=y_test)\n",
    "        #model = xgb.train(params=params,\n",
    "        #                dtrain=dtrain,\n",
    "        #                num_boost_round=num_boost_round, # If training ever reaches 10000 rounds without early stopping, this should be increased\n",
    "        #                early_stopping_rounds=20,\n",
    "        #                evals=[(dval,eval_name)],\n",
    "        #                verbose_eval=20)\n",
    "        #X_test = xgb.DMatrix(data=X_test)\n",
    "        model = XGBRegressor(**params) \n",
    "        model.fit(X_train, y_train) \n",
    "        y_pred = model.predict(X_test) \n",
    "        y_pred = model.predict(X_test) \n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False) \n",
    "        rmse_list.append(rmse)\n",
    "    rmse_avg = sum(rmse_list)/len(rmse_list) / 3600\n",
    "    return sum(rmse_list)/len(rmse_list) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "812da308-69c3-4ea3-be83-969443cf65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Save the study before running the next trial\n",
    "    joblib.dump(study, \"../results/hyper_opt/optuna_studies/study_xgb_no_earlystopping_prime_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H\") + \".pkl\")\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 7)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.1, 0.5)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0, 1)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.5, 1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1)\n",
    "    random_state = 42\n",
    "    \n",
    "    params = {'learning_rate': learning_rate, 'subsample': subsample, 'colsample_bytree': colsample_bytree, \\\n",
    "                  'gamma': gamma, 'max_depth': max_depth, 'n_estimators': n_estimators, 'random_state': random_state}\n",
    "    result = fit_model(params)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3766321-15d2-40e0-a82e-6cdc3fee38c1",
   "metadata": {},
   "source": [
    "### Define parameters used for model training in the objective function below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb47e5-ca40-431b-bd96-2642516a7de7",
   "metadata": {},
   "source": [
    "### Define the objective function.\n",
    "See https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/001_first.html#sphx-glr-tutorial-10-key-features-001-first-py for an introduction to using Optuna for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73b8c2-feb7-4d08-8b5c-3df9b75cdb48",
   "metadata": {},
   "source": [
    "### If continuing a previous study, set `load_study` to `True` and provide the study name.\n",
    "*Note:* Studies are saved in the `studies` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8575c-7002-485d-a89c-8a255ca8aace",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Arrived at 8.8 within 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8003a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_study = True\n",
    "study_name = \"study_xgb_no_earlystopping_20230302-23.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21568a5e-8f94-4035-94c3-77098e4e67a7",
   "metadata": {},
   "source": [
    "### Either load the previous study or create a new study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34ff5265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial until now:\n",
      " Value:  8.514990033892648\n",
      " Params: \n",
      "    n_estimators: 168\n",
      "    max_depth: 7\n",
      "    learning_rate: 0.3968571956999504\n",
      "    gamma: 0.640232768439118\n",
      "    subsample: 0.747747407403972\n",
      "    colsample_bytree: 0.6280085182287491\n"
     ]
    }
   ],
   "source": [
    "if load_study:\n",
    "    study = joblib.load(\"../results/hyper_opt/optuna_studies/\" + study_name)\n",
    "    print(\"Best trial until now:\")\n",
    "    print(\" Value: \", study.best_trial.value)\n",
    "    print(\" Params: \")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "else:\n",
    "    study = optuna.create_study(direction='minimize')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8f6ac9-735c-49ad-abd7-444cab0829e6",
   "metadata": {},
   "source": [
    "### Run the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa16b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-03 00:19:26,209]\u001b[0m Trial 88 finished with value: 8.70028669130951 and parameters: {'n_estimators': 130, 'max_depth': 7, 'learning_rate': 0.38392194201022134, 'gamma': 0.6231499928907753, 'subsample': 0.8152492076837146, 'colsample_bytree': 0.6630512349951684}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 00:22:33,886]\u001b[0m Trial 89 finished with value: 8.889830834577081 and parameters: {'n_estimators': 117, 'max_depth': 7, 'learning_rate': 0.36599444671494896, 'gamma': 0.581910476445027, 'subsample': 0.8087919090745046, 'colsample_bytree': 0.6934003239926613}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 00:26:38,081]\u001b[0m Trial 90 finished with value: 8.758695616559024 and parameters: {'n_estimators': 161, 'max_depth': 7, 'learning_rate': 0.37463229322292996, 'gamma': 0.6193772163317779, 'subsample': 0.8495965117624804, 'colsample_bytree': 0.6625293007158504}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 00:30:41,624]\u001b[0m Trial 91 finished with value: 8.876144786834997 and parameters: {'n_estimators': 162, 'max_depth': 7, 'learning_rate': 0.38236805779481575, 'gamma': 0.6251208781875348, 'subsample': 0.8574593748664491, 'colsample_bytree': 0.6650352412450213}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 00:33:59,572]\u001b[0m Trial 92 finished with value: 8.818954030790884 and parameters: {'n_estimators': 130, 'max_depth': 7, 'learning_rate': 0.3709478178164477, 'gamma': 0.6596741681382123, 'subsample': 0.8171310271859304, 'colsample_bytree': 0.6827725732835657}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 00:37:33,752]\u001b[0m Trial 93 finished with value: 8.806801230931992 and parameters: {'n_estimators': 141, 'max_depth': 7, 'learning_rate': 0.412792224431849, 'gamma': 0.5528832206908908, 'subsample': 0.8463457059914151, 'colsample_bytree': 0.661480077658282}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 00:41:29,709]\u001b[0m Trial 94 finished with value: 8.849703509430734 and parameters: {'n_estimators': 152, 'max_depth': 7, 'learning_rate': 0.35481633650182604, 'gamma': 0.6113422896286558, 'subsample': 0.8325528177036182, 'colsample_bytree': 0.6475779670885958}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 00:44:43,796]\u001b[0m Trial 95 finished with value: 9.510336267662913 and parameters: {'n_estimators': 125, 'max_depth': 7, 'learning_rate': 0.3837539190682081, 'gamma': 0.5883376806540296, 'subsample': 0.7999997252832471, 'colsample_bytree': 0.6401872375546767}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 00:48:51,266]\u001b[0m Trial 96 finished with value: 8.573688073511132 and parameters: {'n_estimators': 156, 'max_depth': 7, 'learning_rate': 0.398327573929002, 'gamma': 0.6706679620514633, 'subsample': 0.820954101917523, 'colsample_bytree': 0.6307221411595163}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 00:52:53,685]\u001b[0m Trial 97 finished with value: 8.768074853852161 and parameters: {'n_estimators': 155, 'max_depth': 7, 'learning_rate': 0.4017881482052013, 'gamma': 0.6825989439938615, 'subsample': 0.8398525297660105, 'colsample_bytree': 0.613373079491781}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 00:56:59,025]\u001b[0m Trial 98 finished with value: 9.11367424808346 and parameters: {'n_estimators': 158, 'max_depth': 7, 'learning_rate': 0.3996095755420252, 'gamma': 0.6331491997460619, 'subsample': 0.8728849875484106, 'colsample_bytree': 0.670685644112976}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 01:00:55,596]\u001b[0m Trial 99 finished with value: 8.714221930248739 and parameters: {'n_estimators': 154, 'max_depth': 7, 'learning_rate': 0.3753660511796181, 'gamma': 0.6759963727482226, 'subsample': 0.8511804338849351, 'colsample_bytree': 0.6147065774640363}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[32m[I 2023-03-03 01:04:58,453]\u001b[0m Trial 100 finished with value: 8.897030180085242 and parameters: {'n_estimators': 155, 'max_depth': 7, 'learning_rate': 0.37589773468363086, 'gamma': 0.6739248010163985, 'subsample': 0.8544048811982252, 'colsample_bytree': 0.6257553140891056}. Best is trial 81 with value: 8.514990033892648.\u001b[0m\n",
      "\u001b[33m[W 2023-03-03 01:05:30,792]\u001b[0m Trial 101 failed with parameters: {'n_estimators': 146, 'max_depth': 7, 'learning_rate': 0.3633633384728825, 'gamma': 0.6057491126568468, 'subsample': 0.8431451139995423, 'colsample_bytree': 0.6138242756490472} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kmenear/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/cy/yhfymg2j61gfl448mb_195qxlvt1vt/T/ipykernel_2129/3729870053.py\", line 14, in objective\n",
      "    result = fit_model(params)\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/cy/yhfymg2j61gfl448mb_195qxlvt1vt/T/ipykernel_2129/1594578321.py\", line 21, in fit_model\n",
      "    model.fit(X_train, y_train)\n",
      "  File \"/Users/kmenear/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/xgboost/core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/kmenear/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/xgboost/sklearn.py\", line 1051, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/Users/kmenear/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/xgboost/core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/kmenear/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/xgboost/training.py\", line 185, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"/Users/kmenear/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/xgboost/core.py\", line 1918, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-03-03 01:05:30,796]\u001b[0m Trial 101 failed with value None.\u001b[0m\n",
      "Exception ignored in: <function Booster.__del__ at 0x115aa0f40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kmenear/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/xgboost/core.py\", line 1651, in __del__\n",
      "    def __del__(self) -> None:\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     10\u001b[0m random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     12\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: learning_rate, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m: subsample, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: colsample_bytree, \\\n\u001b[1;32m     13\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: gamma, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: max_depth, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: n_estimators, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: random_state}\n\u001b[0;32m---> 14\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#dtrain = xgb.DMatrix(data=X_train,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#                     label=y_train)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#dval = xgb.DMatrix(data=X_test,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#                verbose_eval=20)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#X_test = xgb.DMatrix(data=X_test)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams) \n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     22\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test) \n\u001b[1;32m     23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test) \n",
      "File \u001b[0;32m~/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/xgboost/sklearn.py:1051\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m (\n\u001b[1;32m   1043\u001b[0m     model,\n\u001b[1;32m   1044\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1050\u001b[0m )\n\u001b[0;32m-> 1051\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/hpc_runtime_prediction/environment/lib/python3.11/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study.optimize(objective, n_trials=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eb4db7-3339-4ba3-bbdb-e4c0104174b9",
   "metadata": {},
   "source": [
    "### Print the hyperparameter and objective function values for the best trial in the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5efd2871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 168, 'max_depth': 7, 'learning_rate': 0.3968571956999504, 'gamma': 0.640232768439118, 'subsample': 0.747747407403972, 'colsample_bytree': 0.6280085182287491}\n"
     ]
    }
   ],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2cf2e753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.514990033892648\n"
     ]
    }
   ],
   "source": [
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbf872-7a4a-4847-bb00-51034c43dcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
